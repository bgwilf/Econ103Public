%\section{Review of Linear Regression as a Data Summary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%\frametitle{Recall: ``Best Fitting'' Line Through Cloud of Points}
%\begin{figure}
%	\includegraphics[scale = 0.48]{./images/midterms5}
%\end{figure}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%\frametitle{Recall: Regression as a Data Summary}
%\begin{block}{Linear Model}
%$\hat{y} = a + b x$
%\end{block}
%\begin{block}{Choose $a,b$ to Minimize Sum of Squared Vertical Deviations}
%$\displaystyle\sum_{i = 1}^n d_i^2 = \sum_{i=1}^n (y_i - a - b x_i)^2$
%\end{block}
%
%\begin{block}{The Prediction}
%Predict score $\hat{y} = a + b x$ on second midterm for someone with score $x$ on first.
%\end{block}
%
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%\frametitle{Recall: Regression as a Data Summary}
%	\begin{block}{Problem}
%	$$\min_{a,b}  \sum_{i=1}^n (y_i - a - b x_i)^2$$
%\end{block}
%\begin{block}{Solution}
%	\begin{eqnarray*}
%		b &=& \frac{\sum_{i=1}^n \left(y_i - \bar{y}\right)\left(x_i - \bar{x} \right)}{\sum_{i=1}^n \left(x_i - \bar{x}\right)^2} = \frac{s_{xy}}{s_x^2} =  r\frac{s_y}{s_x}\\ \\
%		a &=& \bar{y} - b\bar{x}
%	\end{eqnarray*}
%\end{block}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Population Regression Model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Beyond Regression as a Data Summary}
\small
Based on a sample of Econ 103 students, we made the following graph of handspan against height, and fitted a linear regression:
\begin{center}
\includegraphics[scale = 0.3]{./images/height_handspan_reg}
\end{center}
The estimated slope was about 1.4 inches/cm and the estimated intercept was about 40 inches. \\


\alert{What if anything does this tell us about the relationship between height and handspan \emph{in the population}?}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{The Population Regression Model}

  \begin{block}{Question}
    If we want to predict $Y$ using $X$ in the \emph{population} using a line, how should we choose the slope and intercept?
\end{block}

\begin{block}{Optimization Problem}
  Choose $\beta_0,\beta_1$ to minimize $E[(Y - \beta_0 - \beta_1 X)^2]$
\end{block}

\begin{alertblock}{Solution}
  \vspace{-1em}
  \[
    \beta_1 = \frac{Cov(X,Y)}{Var(X)}, \quad \beta_0 = E[Y] - \beta_1 E[X]
  \]
\end{alertblock}

\alert{\hfill\dots you will derive this as an extension problem.}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{The Regression Error Term: $\varepsilon$}

  \begin{block}{Definition}
    $\varepsilon \equiv Y - \beta_0 - \beta_1 X\quad$ (Hence: $Y = \beta_0 + \beta_1 X + \varepsilon$)
  \end{block}

  \begin{block}{Interpretation}
    $\varepsilon$ is the part of $Y$ that isn't predicted by $X$
  \end{block}

  \begin{block}{Properties}
    \begin{itemize}
      \item $E[\varepsilon] = 0$
      \item $Cov(X,\varepsilon) = 0$
      \item $Var(\varepsilon) = Var(Y) - Cov(X,Y)^2/Var(X)$
    \end{itemize}
  \end{block}

  \alert{\dots using the expressions for $\beta_0$ and $\beta_1$ from the previous slide.}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{The Population Regression Coefficients: $\beta_0, \beta_1$}

\begin{block}{Recall}
  \vspace{-1em}
  \[
    Y = \beta_0 + \beta_1 X + \varepsilon, \quad \beta_1 = \frac{Cov(X,Y)}{Var(X)}, \quad \beta_0 = E[Y] - \beta_1 E[X]
  \]
\end{block}

\begin{block}{Interpretation}
  \begin{itemize}
    \item $\beta_0, \beta_1$ are population parameters: \emph{unknown constants}
    \item If $X = 0$, we predict $Y = \beta_0$.
    \item If two people differ by one unit in $X$, we predict that they will differ by $\beta_1$ units in $Y$.  
  \end{itemize}
\end{block}

	\alert{The only problem is, we don't know $\beta_0, \beta_1$...}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Estimating $\beta_0, \beta_1$}

\begin{block}{Random Sample}
  Observe $(Y_1, X_1), \hdots, (Y_n, X_n) \sim \text{iid}$ with $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$. 
\end{block}
  
\begin{block}{Estimators of $\beta_0, \beta_1$}
\[
  \widehat{\beta}_1 = \frac{S_{XY}}{S_X^2} = \frac{\sum_{i=1}^n (X_i - \bar{X}_n) (Y_i - \bar{Y}_n)}{\sum_{i=1}^n (X_i - \bar{X}_n)^2}, \quad
	\widehat{\beta}_0 = \bar{Y}_n - \widehat{\beta}_1 \bar{X}_n
\]
\end{block}

\vspace{1em}
\alert{Under random sampling, the estimators $(\widehat{\beta}_0, \widehat{\beta}_1)$ have sampling distributions\dots}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Inference for Regression}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Sampling Uncertainty: Pretend the Class is our Population}

\begin{figure}[h]
\centering
\includegraphics[scale = 0.5]{./images/height_handspan_reg}
\caption{Estimated Slope = 1.4, Estimated Intercept = 40}
\end{figure}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Sampling Distribution of Regression Coefficients $\widehat{\beta}_0$ and $\widehat{\beta}_1$}
\vspace{1em}

\begin{center}
\setlength{\unitlength}{1cm}
\begin{picture}(5,7)
\put(-2,6){\framebox(9,1){Choose 25 Students from Class List with Replacement}}



\put(0.5,6){\vector(-1,-1){1.5}}
\put(-2.3,3.7){\framebox(2.5,0.65){Sample 1}}



\put(-1,3.5){\vector(0,-1){1}}
\put(-1.6,1.9){\makebox{\small $\widehat{\beta}_0^{(1)}, \widehat{\beta}_1^{(1)}$}}



\put(2,6){\vector(0,-1){1.5}}
\put(0.7,3.7){\framebox(2.5,0.65){Sample 2}}



\put(2,3.5){\vector(0,-1){1}}
\put(1.4,1.9){\makebox{\small $\widehat{\beta}_0^{(2)}, \widehat{\beta}_1^{(2)}$}}



\put(3.8,4){\makebox{...}}
\put(3.8,2){\makebox{...}}



\put(4.5,6){\vector(1,-1){1.5}}
\put(4.8,3.7){\framebox(2.5,0.65){Sample 1000}}



\put(6,3.5){\vector(0,-1){1}}
\put(5.3,1.9){\makebox{\small $\widehat{\beta}_0^{(1000)}, \widehat{\beta}_1^{(1000)}$}}



\put(-2,0.6){\makebox{\small Repeat 1000 times $\rightarrow$  get 1000 different pairs of estimates}}



\put(-2,0.1){\makebox{\small \alert{Sampling Distribution: long-run relative frequencies}}}

\end{picture}
\end{center}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{1000 Replications, $n=25$}

\begin{figure}[h]
\centering
\includegraphics[scale = 0.6]{./images/reg_lines_sim}

\end{figure}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Population: Intercept = 40, Slope = 1.4}

\begin{figure}[h]
\centering
\includegraphics[scale = 0.55]{./images/reg_sim_hist}

\end{figure}
\vspace{-1em}
\alert{Based on 1000 Replications, $n = 25$}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Inference for Linear Regression}
	\begin{block}{Central Limit Theorem}
		$$\frac{\widehat{\beta} - \beta}{\widehat{SE}(\widehat{\beta})} \approx N(0,1)$$ 
\end{block}

\begin{block}{How to calculate $\widehat{SE}$?}
  R will do this for us, but we won't cover the details in Econ 103. 
  You'll have to wait for Econ 104!
\end{block}
%	\begin{itemize}
%\item Complicated 
%	\begin{itemize}
%\item Depends on variance of errors $\epsilon$ and all predictors in regression. 
%\item We'll look at a few simple examples 
%\item R does this calculation for us 
%\end{itemize}
%\item Requires assumptions about population errors $\epsilon_i$ 
%	\begin{itemize}
%\item Simplest (and R default) is to assume $\epsilon_i \sim iid (0,\sigma^2)$ 
%\item Weaker assumptions in Econ 104 
%\end{itemize}
%
%\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%\frametitle{Intuition for What Effects $SE(\widehat{\beta}_1)$ for Simple Regression}
%
%	$$SE(\widehat{\beta}_1) \approx \frac{\sigma}{\sqrt{n}} \cdot \frac{1}{s_X}$$
%	\begin{itemize}
%	\item $\sigma = SD(\epsilon)$ -- inherent variability of the $Y$, even after controlling for $X$
%	\item $n$ is the sample size
%	\item $s_X$ is the sampling variability of the $X$ observations.
%	\end{itemize}
%\end{frame}
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%I treated the class as our population for the purposes of the simulation experiment but it makes more sense to think of the class as a sample from some population. We'll take this perspective now and think about various inferences we can draw from the height and handspan data using regression.
%\end{frame}
%
%
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Inference for Regression: Predicting Height}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{How to get R to display standard errors?}
  \small
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{data_url} \hlkwb{<-} \hlstr{'http://ditraglia.com/econ103/old_survey.csv'}
\hlstd{survey} \hlkwb{<-} \hlkwd{read.csv}\hlstd{(data_url)}
\hlstd{survey} \hlkwb{<-} \hlkwd{na.omit}\hlstd{(survey)}
\hlstd{reg1} \hlkwb{<-} \hlkwd{lm}\hlstd{(height} \hlopt{~} \hlstd{handspan,} \hlkwc{data} \hlstd{= survey)}
\hlstd{reg1} \hlcom{# Gives estimates but not SE}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = height ~ handspan, data = survey)
## 
## Coefficients:
## (Intercept)     handspan  
##      39.596        1.356
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{\texttt{summary} gives too much information\dots}
  \tiny
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}\hlstd{(reg1)}
\end{alltt}
\begin{verbatim}
## 
## Call:
## lm(formula = height ~ handspan, data = survey)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -10.0680  -2.4238   0.2204   2.7073   7.9657 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  39.5962     3.9596  10.000 1.26e-15 ***
## handspan      1.3558     0.1898   7.143 4.20e-10 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.556 on 78 degrees of freedom
## Multiple R-squared:  0.3955,	Adjusted R-squared:  0.3877 
## F-statistic: 51.03 on 1 and 78 DF,  p-value: 4.201e-10
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{\texttt{display}, from my website, cuts the less important info\dots} 
  \small
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{source}\hlstd{(}\hlstr{'http://ditraglia.com/econ103/display.R'}\hlstd{)}
\hlkwd{display}\hlstd{(reg1)}
\end{alltt}
\begin{verbatim}
## lm(formula = height ~ handspan, data = survey)
##             coef.est coef.se
## (Intercept) 39.60     3.96  
## handspan     1.36     0.19  
## ---
## n = 80, k = 2
## residual sd = 3.56, R-Squared = 0.40
\end{verbatim}
\end{kframe}
\end{knitrout}
\alert{We will learn what everything in this table means\dots}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Regression with only an intercept: sample mean}
  \framesubtitle{See Review Problem \#46}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{reg2} \hlkwb{<-} \hlkwd{lm}\hlstd{(height} \hlopt{~} \hlnum{1}\hlstd{,} \hlkwc{data} \hlstd{= survey)}
\hlkwd{display}\hlstd{(reg2)}
\end{alltt}
\begin{verbatim}
## lm(formula = height ~ 1, data = survey)
##             coef.est coef.se
## (Intercept) 67.74     0.51  
## ---
## n = 80, k = 1
## residual sd = 4.54, R-Squared = 0.00
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Dummy Variable (aka Binary Variable)}
 
A predictor variable that takes on only two values: 0 or 1. Used to represent two categories, e.g.\ Male/Female.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]
  \frametitle{Regression with intercept \& dummy variable: Male/Female}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{reg3} \hlkwb{<-} \hlkwd{lm}\hlstd{(height} \hlopt{~} \hlstd{sex,} \hlkwc{data} \hlstd{= survey)}
\hlkwd{display}\hlstd{(reg3)}
\end{alltt}
\begin{verbatim}
## lm(formula = height ~ sex, data = survey)
##             coef.est coef.se
## (Intercept) 64.46     0.56  
## sexMale      6.10     0.76  
## ---
## n = 80, k = 2
## residual sd = 3.38, R-Squared = 0.45
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Height \& Handspan Regression}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{display}\hlstd{(reg1)}
\end{alltt}
\begin{verbatim}
## lm(formula = height ~ handspan, data = survey)
##             coef.est coef.se
## (Intercept) 39.60     3.96  
## handspan     1.36     0.19  
## ---
## n = 80, k = 2
## residual sd = 3.56, R-Squared = 0.40
\end{verbatim}
\end{kframe}
\end{knitrout}
\alert{What are \texttt{n}, \texttt{k}, \texttt{residual sd} and \texttt{R-Squared}?}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Residual Standard Deviation and $R^2$}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Fitted Values and Residuals}

\begin{block}{Fitted Value $\widehat{y}_i$}
Predicted $y$-value for person $i$ given her $x$-variables using estimated regression coefficients: \alert{$\widehat{y}_i = \widehat{\beta}_0 + \widehat{\beta}_1 x_{i}$}
\end{block}


\begin{block}{Residual $\widehat{\epsilon}_i$}
  Person i's \emph{vertical deviation} from regression line: \alert{$\widehat{\epsilon}_i = y_i - \widehat{y}_i$}. 
\end{block}

\vspace{1em}
\alert{The residuals are \emph{stand-ins} for the unobserved errors $\epsilon_i$.}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Residual Standard Deviation: $\widehat{\sigma}$}
	\begin{itemize}
    \item Idea: use residuals $\widehat{\epsilon}_i$ to estimate $\sigma$
	$$\widehat{\sigma}  = \sqrt{\frac{\sum_{i=1}^n \widehat{\epsilon}_i^2}{n -k}}$$ 
		\item Measures avg.\ distance of $y_i$ from regression line.
				\begin{itemize}
					\item E.g.\ if $Y$ is points scored on a test and $\widehat{\sigma}=16$, the regression predicts to an accuracy of about 16 points. 
				\end{itemize}
	\item Same units as $Y$ (Exam practice: verify this) 
	\item Denominator  ($n-k$) = (\# Datapoints - \# of $X$ variables) 
	\end{itemize}

\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{$R^2$: Proportion of $Var(Y)$ ``Explained'' by the Regression}
\[
  R^2 = 1 - \frac{\sum_{i=1}^n \widehat{\varepsilon}_i^2}{\sum_{i=1}^n (y_i - \bar{y})^2} \approx 1 - \frac{\widehat{\sigma}^2}{s_y^2}
\]

		\begin{itemize}
      \item Unitless, between 0 and 1
      \item Higher value $\implies$ greater proportion of variance ``explained''
      \item Harder to interpret than $\widehat{\sigma}$
      \item Special Case:
        \begin{itemize}
          \item In a regression with a single $X$-variable (``simple'' regression) can show that $R^2 = r_{xy}^2$ hence the name ``R-squared''
        \end{itemize}
    \end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\footnotesize
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{display}\hlstd{(reg1)}
\end{alltt}
\begin{verbatim}
## lm(formula = height ~ handspan, data = survey)
##             coef.est coef.se
## (Intercept) 39.60     3.96  
## handspan     1.36     0.19  
## ---
## n = 80, k = 2
## residual sd = 3.56, R-Squared = 0.40
\end{verbatim}
\begin{alltt}
\hlkwd{cor}\hlstd{(survey}\hlopt{$}\hlstd{height, survey}\hlopt{$}\hlstd{handspan)}\hlopt{^}\hlnum{2}
\end{alltt}
\begin{verbatim}
## [1] 0.3954669
\end{verbatim}
\begin{alltt}
\hlkwd{sqrt}\hlstd{(}\hlkwd{sum}\hlstd{(reg1}\hlopt{$}\hlstd{residuals}\hlopt{^}\hlnum{2}\hlstd{)} \hlopt{/} \hlstd{(}\hlnum{80} \hlopt{-} \hlnum{2}\hlstd{))}
\end{alltt}
\begin{verbatim}
## [1] 3.555941
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\frametitle{Which Gives Better Predictions: Sex or Handspan?}

\scriptsize
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## lm(formula = height ~ handspan, data = survey)
##             coef.est coef.se
## (Intercept) 39.60     3.96  
## handspan     1.36     0.19  
## ---
## n = 80, k = 2
## residual sd = 3.56, R-Squared = 0.40
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## lm(formula = height ~ sex, data = survey)
##             coef.est coef.se
## (Intercept) 64.46     0.56  
## sexMale      6.10     0.76  
## ---
## n = 80, k = 2
## residual sd = 3.38, R-Squared = 0.45
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multiple Regression}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Simple vs.\ Multiple Regression}
\begin{block}{Terminology}
$Y$ is the ``outcome'' and $X$ is the ``predictor.''
\end{block}

\begin{block}{Simple Regression}
One predictor variable: $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$
\end{block}
\begin{block}{Multiple Regression}
More than one predictor variable: $Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} +  \hdots + \beta_k X_{ik} + \epsilon_i$
\end{block}


%\begin{itemize}
%	\item In both cases $\epsilon_1, \epsilon_2, \hdots, \epsilon_n \sim \mbox{iid} (0,\sigma^2)$
%	\item Multiple regression coefficient estimates $\widehat{\beta}_1, \widehat{\beta}_1, \hdots, \widehat{\beta}_k$ calculated by minimizing  sum of squared vertical deviations, but formula requires linear algebra so we won't cover it.
%\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Multiple Regression}
$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} +  \hdots + \beta_k X_{ik} + \epsilon_i$$

\begin{block}{Ceteris Paribus Interpretation}
  If two individuals differ by one unit in $X_j$ but have the \emph{same values for all other predictors}, we predict they will differ by $\beta_j$ units in $Y$.
\end{block}

\begin{block}{Estimating $\beta_0, \beta_1, \dots, \beta_k$}
  The formulas require matrix algebra: R will do it for us.
\end{block}

\begin{block}{Inference for Multiple Regression}
$\displaystyle\frac{\widehat{\beta}_j - \beta_j}{\widehat{SE}(\widehat{\beta}_j)} \approx N(0,1)$ if $n$ is large. 
R will calculate the SE for us.
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Multiple Regression Example: Sex and Handspan}
\small
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{reg4} \hlkwb{<-} \hlkwd{lm}\hlstd{(height} \hlopt{~} \hlstd{sex} \hlopt{+} \hlstd{handspan,} \hlkwc{data} \hlstd{= survey)}
\hlkwd{display}\hlstd{(reg4)}
\end{alltt}
\begin{verbatim}
## lm(formula = height ~ sex + handspan, data = survey)
##             coef.est coef.se
## (Intercept) 49.95     4.16  
## sexMale      4.18     0.89  
## handspan     0.75     0.21  
## ---
## n = 80, k = 3
## residual sd = 3.16, R-Squared = 0.53
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

