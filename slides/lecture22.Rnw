%\section{Review of Linear Regression as a Data Summary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%\frametitle{Recall: ``Best Fitting'' Line Through Cloud of Points}
%\begin{figure}
%	\includegraphics[scale = 0.48]{./images/midterms5}
%\end{figure}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%\frametitle{Recall: Regression as a Data Summary}
%\begin{block}{Linear Model}
%$\hat{y} = a + b x$
%\end{block}
%\begin{block}{Choose $a,b$ to Minimize Sum of Squared Vertical Deviations}
%$\displaystyle\sum_{i = 1}^n d_i^2 = \sum_{i=1}^n (y_i - a - b x_i)^2$
%\end{block}
%
%\begin{block}{The Prediction}
%Predict score $\hat{y} = a + b x$ on second midterm for someone with score $x$ on first.
%\end{block}
%
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%\frametitle{Recall: Regression as a Data Summary}
%	\begin{block}{Problem}
%	$$\min_{a,b}  \sum_{i=1}^n (y_i - a - b x_i)^2$$
%\end{block}
%\begin{block}{Solution}
%	\begin{eqnarray*}
%		b &=& \frac{\sum_{i=1}^n \left(y_i - \bar{y}\right)\left(x_i - \bar{x} \right)}{\sum_{i=1}^n \left(x_i - \bar{x}\right)^2} = \frac{s_{xy}}{s_x^2} =  r\frac{s_y}{s_x}\\ \\
%		a &=& \bar{y} - b\bar{x}
%	\end{eqnarray*}
%\end{block}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Population Regression Model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Beyond Regression as a Data Summary}
\small
Based on a sample of Econ 103 students, we made the following graph of handspan against height, and fitted a linear regression:
\begin{center}
\includegraphics[scale = 0.3]{./images/height_handspan_reg}
\end{center}
The estimated slope was about 1.4 inches/cm and the estimated intercept was about 40 inches. \\


\alert{What if anything does this tell us about the relationship between height and handspan \emph{in the population}?}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{The Population Regression Model}

  \begin{block}{Question}
    If we want to predict $Y$ using $X$ in the \emph{population} using a line, how should we choose the slope and intercept?
\end{block}

\begin{block}{Optimization Problem}
  Choose $\beta_0,\beta_1$ to minimize $E[(Y - \beta_0 - \beta_1 X)^2]$
\end{block}

\begin{alertblock}{Solution}
  \vspace{-1em}
  \[
    \beta_1 = \frac{Cov(X,Y)}{Var(X)}, \quad \beta_0 = E[Y] - \beta_1 E[X]
  \]
\end{alertblock}

\alert{\hfill\dots you will derive this as an extension problem.}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{The Regression Error Term: $\varepsilon$}

  \begin{block}{Definition}
    $\varepsilon \equiv Y - \beta_0 - \beta_1 X\quad$ (Hence: $Y = \beta_0 + \beta_1 X + \varepsilon$)
  \end{block}

  \begin{block}{Interpretation}
    $\varepsilon$ is the part of $Y$ that isn't predicted by $X$
  \end{block}

  \begin{block}{Properties}
    \begin{itemize}
      \item $E[\varepsilon] = 0$
      \item $Cov(X,\varepsilon) = 0$
      \item $Var(\varepsilon) = Var(Y) - Cov(X,Y)^2/Var(X)$
    \end{itemize}
  \end{block}

  \alert{\dots using the expressions for $\beta_0$ and $\beta_1$ from the previous slide.}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{The Population Regression Coefficients: $\beta_0, \beta_1$}

\begin{block}{Recall}
  \vspace{-1em}
  \[
    Y = \beta_0 + \beta_1 X + \varepsilon, \quad \beta_1 = \frac{Cov(X,Y)}{Var(X)}, \quad \beta_0 = E[Y] - \beta_1 E[X]
  \]
\end{block}

\begin{block}{Interpretation}
  \begin{itemize}
    \item $\beta_0, \beta_1$ are population parameters: \emph{unknown constants}
    \item If $X = 0$, we predict $Y = \beta_0$.
    \item If two people differ by one unit in $X$, we predict that they will differ by $\beta_1$ units in $Y$.  
  \end{itemize}
\end{block}

	\alert{The only problem is, we don't know $\beta_0, \beta_1$...}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Estimating $\beta_0, \beta_1$}

\begin{block}{Random Sample}
  Observe $(Y_1, X_1), \hdots, (Y_n, X_n) \sim \text{iid}$ with $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$. 
\end{block}
  
\begin{block}{Estimators of $\beta_0, \beta_1$}
\[
  \widehat{\beta}_1 = \frac{S_{XY}}{S_X^2} = \frac{\sum_{i=1}^n (X_i - \bar{X}_n) (Y_i - \bar{Y}_n)}{\sum_{i=1}^n (X_i - \bar{X}_n)^2}, \quad
	\widehat{\beta}_0 = \bar{Y}_n - \widehat{\beta}_1 \bar{X}_n
\]
\end{block}

\vspace{1em}
\alert{Under random sampling, the estimators $(\widehat{\beta}_0, \widehat{\beta}_1)$ have sampling distributions\dots}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Inference for Regression}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Sampling Uncertainty: Pretend the Class is our Population}

\begin{figure}[h]
\centering
\includegraphics[scale = 0.5]{./images/height_handspan_reg}
\caption{Estimated Slope = 1.4, Estimated Intercept = 40}
\end{figure}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Sampling Distribution of Regression Coefficients $\widehat{\beta}_0$ and $\widehat{\beta}_1$}
\vspace{1em}

\begin{center}
\setlength{\unitlength}{1cm}
\begin{picture}(5,7)
\put(-2,6){\framebox(9,1){Choose 25 Students from Class List with Replacement}}



\put(0.5,6){\vector(-1,-1){1.5}}
\put(-2.3,3.7){\framebox(2.5,0.65){Sample 1}}



\put(-1,3.5){\vector(0,-1){1}}
\put(-1.6,1.9){\makebox{\small $\widehat{\beta}_0^{(1)}, \widehat{\beta}_1^{(1)}$}}



\put(2,6){\vector(0,-1){1.5}}
\put(0.7,3.7){\framebox(2.5,0.65){Sample 2}}



\put(2,3.5){\vector(0,-1){1}}
\put(1.4,1.9){\makebox{\small $\widehat{\beta}_0^{(2)}, \widehat{\beta}_1^{(2)}$}}



\put(3.8,4){\makebox{...}}
\put(3.8,2){\makebox{...}}



\put(4.5,6){\vector(1,-1){1.5}}
\put(4.8,3.7){\framebox(2.5,0.65){Sample 1000}}



\put(6,3.5){\vector(0,-1){1}}
\put(5.3,1.9){\makebox{\small $\widehat{\beta}_0^{(1000)}, \widehat{\beta}_1^{(1000)}$}}



\put(-2,0.6){\makebox{\small Repeat 1000 times $\rightarrow$  get 1000 different pairs of estimates}}



\put(-2,0.1){\makebox{\small \alert{Sampling Distribution: long-run relative frequencies}}}

\end{picture}
\end{center}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{1000 Replications, $n=25$}

\begin{figure}[h]
\centering
\includegraphics[scale = 0.6]{./images/reg_lines_sim}

\end{figure}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Population: Intercept = 40, Slope = 1.4}

\begin{figure}[h]
\centering
\includegraphics[scale = 0.55]{./images/reg_sim_hist}

\end{figure}
\vspace{-1em}
\alert{Based on 1000 Replications, $n = 25$}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Inference for Linear Regression}
	\begin{block}{Central Limit Theorem}
		$$\frac{\widehat{\beta} - \beta}{\widehat{SE}(\widehat{\beta})} \approx N(0,1)$$ 
\end{block}

\begin{block}{How to calculate $\widehat{SE}$?}
  R will do this for us, but we won't cover the details in Econ 103. 
  You'll have to wait for Econ 104!
\end{block}
%	\begin{itemize}
%\item Complicated 
%	\begin{itemize}
%\item Depends on variance of errors $\epsilon$ and all predictors in regression. 
%\item We'll look at a few simple examples 
%\item R does this calculation for us 
%\end{itemize}
%\item Requires assumptions about population errors $\epsilon_i$ 
%	\begin{itemize}
%\item Simplest (and R default) is to assume $\epsilon_i \sim iid (0,\sigma^2)$ 
%\item Weaker assumptions in Econ 104 
%\end{itemize}
%
%\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%\frametitle{Intuition for What Effects $SE(\widehat{\beta}_1)$ for Simple Regression}
%
%	$$SE(\widehat{\beta}_1) \approx \frac{\sigma}{\sqrt{n}} \cdot \frac{1}{s_X}$$
%	\begin{itemize}
%	\item $\sigma = SD(\epsilon)$ -- inherent variability of the $Y$, even after controlling for $X$
%	\item $n$ is the sample size
%	\item $s_X$ is the sampling variability of the $X$ observations.
%	\end{itemize}
%\end{frame}
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%I treated the class as our population for the purposes of the simulation experiment but it makes more sense to think of the class as a sample from some population. We'll take this perspective now and think about various inferences we can draw from the height and handspan data using regression.
%\end{frame}
%
%
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Inference for Regression: Predicting Height}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{How to get R to display standard errors?}
  \small
<<load_data>>=
data_url <- 'http://ditraglia.com/econ103/old_survey.csv'
survey <- read.csv(data_url)
survey <- na.omit(survey)
reg1 <- lm(height ~ handspan, data = survey)
reg1 # Gives estimates but not SE
@
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{\texttt{summary} gives too much information\dots}
  \tiny
<<summary_too_much>>=
summary(reg1)
@
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{\texttt{display}, from my website, cuts the less important info\dots} 
  \small
<<display_just_right>>=
source('http://ditraglia.com/econ103/display.R')
display(reg1)
@
\alert{We will learn what everything in this table means\dots}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Regression with only an intercept: sample mean}
  \framesubtitle{See Review Problem \#46}
<<only_intercept>>=
reg2 <- lm(height ~ 1, data = survey)
display(reg2)
@
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Dummy Variable (aka Binary Variable)}
 
A predictor variable that takes on only two values: 0 or 1. Used to represent two categories, e.g.\ Male/Female.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]
  \frametitle{Regression with intercept \& dummy variable: Male/Female}
<<dummy>>=
reg3 <- lm(height ~ sex, data = survey)
display(reg3)
@
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Height \& Handspan Regression}
<<height_handspan_again>>=
display(reg1)
@
\alert{What are \texttt{n}, \texttt{k}, \texttt{residual sd} and \texttt{R-Squared}?}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Residual Standard Deviation and $R^2$}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Fitted Values and Residuals}

\begin{block}{Fitted Value $\widehat{y}_i$}
Predicted $y$-value for person $i$ given her $x$-variables using estimated regression coefficients: \alert{$\widehat{y}_i = \widehat{\beta}_0 + \widehat{\beta}_1 x_{i}$}
\end{block}


\begin{block}{Residual $\widehat{\epsilon}_i$}
  Person i's \emph{vertical deviation} from regression line: \alert{$\widehat{\epsilon}_i = y_i - \widehat{y}_i$}. 
\end{block}

\vspace{1em}
\alert{The residuals are \emph{stand-ins} for the unobserved errors $\epsilon_i$.}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Residual Standard Deviation: $\widehat{\sigma}$}
	\begin{itemize}
    \item Idea: use residuals $\widehat{\epsilon}_i$ to estimate $\sigma$
	$$\widehat{\sigma}  = \sqrt{\frac{\sum_{i=1}^n \widehat{\epsilon}_i^2}{n -k}}$$ 
		\item Measures avg.\ distance of $y_i$ from regression line.
				\begin{itemize}
					\item E.g.\ if $Y$ is points scored on a test and $\widehat{\sigma}=16$, the regression predicts to an accuracy of about 16 points. 
				\end{itemize}
	\item Same units as $Y$ (Exam practice: verify this) 
	\item Denominator  ($n-k$) = (\# Datapoints - \# of $X$ variables) 
	\end{itemize}

\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{$R^2$: Proportion of $Var(Y)$ ``Explained'' by the Regression}
\[
  R^2 = 1 - \frac{\sum_{i=1}^n \widehat{\varepsilon}_i^2}{\sum_{i=1}^n (y_i - \bar{y})^2} \approx 1 - \frac{\widehat{\sigma}^2}{s_y^2}
\]

		\begin{itemize}
      \item Unitless, between 0 and 1
      \item Higher value $\implies$ greater proportion of variance ``explained''
      \item Harder to interpret than $\widehat{\sigma}$
      \item Special Case:
        \begin{itemize}
          \item In a regression with a single $X$-variable (``simple'' regression) can show that $R^2 = r_{xy}^2$ hence the name ``R-squared''
        \end{itemize}
    \end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\footnotesize
<<>>=
display(reg1)
cor(survey$height, survey$handspan)^2
sqrt(sum(reg1$residuals^2) / (80 - 2))
@
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
\frametitle{Which Gives Better Predictions: Sex or Handspan?}

\scriptsize
<<echo = FALSE>>=
display(reg1)
@

<<echo = FALSE>>=
display(reg3)
@

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Multiple Regression}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Simple vs.\ Multiple Regression}
\begin{block}{Terminology}
$Y$ is the ``outcome'' and $X$ is the ``predictor.''
\end{block}

\begin{block}{Simple Regression}
One predictor variable: $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$
\end{block}
\begin{block}{Multiple Regression}
More than one predictor variable: $Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} +  \hdots + \beta_k X_{ik} + \epsilon_i$
\end{block}


%\begin{itemize}
%	\item In both cases $\epsilon_1, \epsilon_2, \hdots, \epsilon_n \sim \mbox{iid} (0,\sigma^2)$
%	\item Multiple regression coefficient estimates $\widehat{\beta}_1, \widehat{\beta}_1, \hdots, \widehat{\beta}_k$ calculated by minimizing  sum of squared vertical deviations, but formula requires linear algebra so we won't cover it.
%\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Multiple Regression}
$$Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} +  \hdots + \beta_k X_{ik} + \epsilon_i$$

\begin{block}{Ceteris Paribus Interpretation}
  If two individuals differ by one unit in $X_j$ but have the \emph{same values for all other predictors}, we predict they will differ by $\beta_j$ units in $Y$.
\end{block}

\begin{block}{Estimating $\beta_0, \beta_1, \dots, \beta_k$}
  The formulas require matrix algebra: R will do it for us.
\end{block}

\begin{block}{Inference for Multiple Regression}
$\displaystyle\frac{\widehat{\beta}_j - \beta_j}{\widehat{SE}(\widehat{\beta}_j)} \approx N(0,1)$ if $n$ is large. 
R will calculate the SE for us.
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]
  \frametitle{Multiple Regression Example: Sex and Handspan}
\small
<<sex_and_handspan>>=
reg4 <- lm(height ~ sex + handspan, data = survey)
display(reg4)
@
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

