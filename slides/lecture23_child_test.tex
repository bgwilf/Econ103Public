\documentclass[12pt]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage[margin = 1.2in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{color}
\usepackage{hyperref}

\author{Econ 103 -- Lecture 23}
\title{Empirical Example: Child Test Scores} 
\date{}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\maketitle

\section*{Introduction}
This is our final lecture!
Today we're going to bring together all the skills you've learned over the course of the semester to work through a real-world example in which we try to predict which children are most at-risk of low test scores based on information about their mothers.
Although this is a very simple example, regressions like these are frequently used to inform policy.
For example, if we want to design an intervention to target disadvantaged children, it would be very valuable to be able to predict which children are most in need \emph{before} we see their test scores at age three since the interventions that make the greatest impact are those adminstered in early childhood.

\section*{Getting Started}
To make R's regression output easier to read, we'll the \texttt{display} function that I discussed in our previous lecture:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{source}\hlstd{(}\hlstr{"http://ditraglia.com/econ103/display.R"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
You'll need to re-run this if you restart R and want to continue using \texttt{display}.

To start, let's load the student test score data from my website.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{data_url} \hlkwb{<-} \hlstr{"http://ditraglia.com/econ103/child_test_data.csv"}
\hlstd{child} \hlkwb{<-} \hlkwd{read.csv}\hlstd{(data_url)}
\end{alltt}
\end{kframe}
\end{knitrout}
Now let's take a quick look at the dataset
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{head}\hlstd{(child)}
\end{alltt}
\begin{verbatim}
##   kid.score mom.hs    mom.iq mom.age
## 1        65      1 121.11753      27
## 2        98      1  89.36188      25
## 3        85      1 115.44316      27
## 4        83      1  99.44964      25
## 5       115      1  92.74571      27
## 6        98      0 107.90184      18
\end{verbatim}
\end{kframe}
\end{knitrout}
Here's a description of all of the columns in this dataframe:
\begin{table}[h]
\centering
  \begin{tabular}{|ll|}
  \hline
		Variable Name & Description\\
		\hline
		\texttt{kid.score}& Child's Test Score at Age 3\\
		\texttt{mom.age}&Age of Mother at Birth of Child\\
		\texttt{mom.hs}& Mother Completed High School? (1 = Yes)\\
		\texttt{mom.iq}& Mother's IQ Score\\
    \hline
	\end{tabular}
\end{table}
As you can see, we have a lot of information here! 
For today, we'll only use the columns \texttt{kid.score}, \texttt{mom.hs} and \texttt{mom.iq}.
In the extension problem for this lecture you'll look at \texttt{mom.age},

\section*{First Regression: \texttt{mom.hs}}
This regression compares average test scores of children whose mother completed high school to those whose mother didn't.
Here, \texttt{mom.hs} is a \emph{dummy variable}: it takes on the value 1 if that child's mother completed high school, 0 otherwise.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{reg1} \hlkwb{<-} \hlkwd{lm}\hlstd{(kid.score} \hlopt{~} \hlstd{mom.hs,} \hlkwc{data} \hlstd{= child)}
\hlkwd{display}\hlstd{(reg1)}
\end{alltt}
\begin{verbatim}
## lm(formula = kid.score ~ mom.hs, data = child)
##             coef.est coef.se
## (Intercept) 77.55     2.06  
## mom.hs      11.77     2.32  
## ---
## n = 434, k = 2
## residual sd = 19.85, R-Squared = 0.06
\end{verbatim}
\end{kframe}
\end{knitrout}
Rounding, we can summarize these regression results as follows:
  $$\mbox{\texttt{kid.score}} = 78 + 12 \cdot \mbox{\texttt{mom.hs}} + \mbox{error} $$
Since \texttt{mom.hs} is a dummy variable, taking on the value 1 if a child's mother completed high school and 0 otherwise, this regression is the \emph{same thing} as comparing the mean test scores of two groups: those whose mother completed high school and those whose mother didn't! 
    \begin{eqnarray*}
      (\mbox{\texttt{mom.hs}} = 1) \Rightarrow \texttt{kid.score} &=&78 + 12 \cdot 1 +\mbox{error}\\
      &=& 90+\mbox{error} \\\\
    (\mbox{\texttt{mom.hs}} = 0) \Rightarrow\texttt{kid.score} &=& 78 + 12 \cdot 0 +\mbox{error}\\
    &=& 78 + \mbox{error}
    \end{eqnarray*}
The difference of means simply equals the coefficient on \texttt{mom.hs}, namely $12$.
Creating a confidence interval for this difference of means is easy, since R has already calculated the required standard error for us. Rounding, this value is approximately $2.3$, so our approximate 95\% confidence interval for the difference of means (the coefficient on \texttt{mom.hs}) is $12 \pm 4.6$, in other words $(7.4, 16.6)$. 
Since this interval doesn't include zero, we would reject the null that children whose mothers completed high school have the same test scores on average as those whose mothers didn't against the two-sided alternative at the 5\% significance level. 
It looks like children whose mothers completed high school do better on this test.

\section*{Second Regression: \texttt{mom.iq}}
Now let's try something different.
We'll use mother IQ to predict child test scores.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{reg2} \hlkwb{<-} \hlkwd{lm}\hlstd{(kid.score} \hlopt{~} \hlstd{mom.iq,} \hlkwc{data} \hlstd{= child)}
\hlkwd{display}\hlstd{(reg2)}
\end{alltt}
\begin{verbatim}
## lm(formula = kid.score ~ mom.iq, data = child)
##             coef.est coef.se
## (Intercept) 25.80     5.92  
## mom.iq       0.61     0.06  
## ---
## n = 434, k = 2
## residual sd = 18.27, R-Squared = 0.20
\end{verbatim}
\end{kframe}
\end{knitrout}
Rounding, we can summarize the results as follows:
  $$\mbox{\texttt{kid.score}} = 26 + 0.6 \cdot \mbox{\texttt{mom.iq}} + \mbox{error} $$
The intercept in this model is not interpretable: it is the predicted test score for a child whose mother has an IQ of zero! 
The coefficient on \texttt{mom.iq} is meaningful, however. 
If we compare two groups of students who differ by one point in mothers' IQ, we would predict that the group with higher mother IQ will score $0.6$ points higher, on average.

We can plot the data as follows
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(child}\hlopt{$}\hlstd{mom.iq, child}\hlopt{$}\hlstd{kid.score,} \hlkwc{pch} \hlstd{=} \hlnum{20}\hlstd{,} \hlkwc{xlab} \hlstd{=} \hlstr{'Mother IQ Score'}\hlstd{,}
     \hlkwc{ylab} \hlstd{=} \hlstr{'Child Test Score'}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-6-1} 

\end{knitrout}
To add the regression line, we need to extract the slope and intercept from the fitted regression:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(child}\hlopt{$}\hlstd{mom.iq, child}\hlopt{$}\hlstd{kid.score,} \hlkwc{pch} \hlstd{=} \hlnum{20}\hlstd{,} \hlkwc{xlab} \hlstd{=} \hlstr{'Mother IQ Score'}\hlstd{,}
     \hlkwc{ylab} \hlstd{=} \hlstr{'Child Test Score'}\hlstd{)}
\hlkwd{abline}\hlstd{(}\hlkwd{coef}\hlstd{(reg2))}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-7-1} 

\end{knitrout}

\section*{Third Regression: \texttt{mom.hs} and \texttt{mom.iq}}
Now we'll fit a regression with both \texttt{mom.hs} and \texttt{mom.iq}.
It turns out that this allows the regression line to have a different \emph{intercept} depending on whether a child's mother completed high school.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{reg3} \hlkwb{<-} \hlkwd{lm}\hlstd{(kid.score} \hlopt{~} \hlstd{mom.hs} \hlopt{+} \hlstd{mom.iq,} \hlkwc{data} \hlstd{= child)}
\hlkwd{display}\hlstd{(reg3)}
\end{alltt}
\begin{verbatim}
## lm(formula = kid.score ~ mom.hs + mom.iq, data = child)
##             coef.est coef.se
## (Intercept) 25.73     5.88  
## mom.hs       5.95     2.21  
## mom.iq       0.56     0.06  
## ---
## n = 434, k = 3
## residual sd = 18.14, R-Squared = 0.21
\end{verbatim}
\end{kframe}
\end{knitrout}
Rounding, we can summarize the fitted model as follows:
  $$\mbox{\texttt{kid.score}} = 26 + 6 \cdot \mbox{\texttt{mom.hs}} +0.6 \cdot \mbox{\texttt{mom.iq}} + \mbox{error} $$
  Since \texttt{mom.hs} is binary, this is equivalent to letting each group have a regression line with a \emph{different intercept} but the same slope:
  \begin{eqnarray*}
    (\texttt{mom.hs = 1})\Rightarrow \texttt{kid.score} &=& 26 + 6 \cdot 1 + 0.6\cdot \texttt{mom.iq} + \mbox{error}\\
      &=& 32 + 0.6\cdot \texttt{mom.iq} + \mbox{error}\\ \\
    (\texttt{mom.hs = 0})\Rightarrow \texttt{kid.score} &=& 26 + 6 \cdot 0 + 0.6\cdot \texttt{mom.iq} + \mbox{error}\\
      &=&26 + 0.6\cdot \texttt{mom.iq} + \mbox{error}
  \end{eqnarray*}
In this case the intercept is not interpretable: it corresponds to the average test score for children whose mother did not complete high school and have a zero IQ! The other two coefficients, however, are meaningful. The coefficient on \texttt{mom.hs} compares test scores for children whose mothers have the same IQ but differ in whether or not they completed high school. The coefficient on \texttt{mom.iq} compares children whose mothers have the same value of \texttt{mom.iq} but differ in IQ by one point. 

We can plot the data and fitted models as follows. We'll plot the children whose mothers went to high school in \emph{gray} and those whose mothers didn't in \emph{black}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{colors} \hlkwb{<-} \hlkwd{ifelse}\hlstd{(child}\hlopt{$}\hlstd{mom.hs} \hlopt{==} \hlnum{0}\hlstd{,} \hlstr{"black"}\hlstd{,} \hlstr{"gray"}\hlstd{)}
\hlkwd{plot}\hlstd{(child}\hlopt{$}\hlstd{mom.iq, child}\hlopt{$}\hlstd{kid.score,} \hlkwc{pch} \hlstd{=} \hlnum{20}\hlstd{,} \hlkwc{xlab} \hlstd{=} \hlstr{'Mother IQ Score'}\hlstd{,}
     \hlkwc{ylab} \hlstd{=} \hlstr{'Child Test Score'}\hlstd{,} \hlkwc{col} \hlstd{= colors)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-9-1} 

\end{knitrout}
To add the fitted regression lines we need to extract the common slope as well as the intercept for \emph{each group}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{coef}\hlstd{(reg3)}
\end{alltt}
\begin{verbatim}
## (Intercept)      mom.hs      mom.iq 
##   25.731538    5.950117    0.563906
\end{verbatim}
\begin{alltt}
\hlstd{b_both} \hlkwb{<-} \hlkwd{coef}\hlstd{(reg3)[}\hlnum{3}\hlstd{]}
\hlstd{a_HS} \hlkwb{<-} \hlkwd{coef}\hlstd{(reg3)[}\hlnum{1}\hlstd{]} \hlopt{+} \hlkwd{coef}\hlstd{(reg3)[}\hlnum{2}\hlstd{]}
\hlstd{a_no_HS} \hlkwb{<-} \hlkwd{coef}\hlstd{(reg3)[}\hlnum{1}\hlstd{]}
\end{alltt}
\end{kframe}
\end{knitrout}
Now we plot the results alongside the two regression lines:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{colors} \hlkwb{<-} \hlkwd{ifelse}\hlstd{(child}\hlopt{$}\hlstd{mom.hs} \hlopt{==} \hlnum{0}\hlstd{,} \hlstr{"black"}\hlstd{,} \hlstr{"gray"}\hlstd{)}
\hlkwd{plot}\hlstd{(child}\hlopt{$}\hlstd{mom.iq, child}\hlopt{$}\hlstd{kid.score,} \hlkwc{pch} \hlstd{=} \hlnum{20}\hlstd{,} \hlkwc{xlab} \hlstd{=} \hlstr{'Mother IQ Score'}\hlstd{,}
     \hlkwc{ylab} \hlstd{=} \hlstr{'Child Test Score'}\hlstd{,} \hlkwc{col} \hlstd{= colors)}
\hlkwd{abline}\hlstd{(}\hlkwc{a} \hlstd{= a_HS,} \hlkwc{b} \hlstd{= b_both,} \hlkwc{col} \hlstd{=} \hlstr{'gray'}\hlstd{)}
\hlkwd{abline}\hlstd{(}\hlkwc{a} \hlstd{= a_no_HS,} \hlkwc{b} \hlstd{= b_both,} \hlkwc{col} \hlstd{=} \hlstr{'black'}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-11-1} 

\end{knitrout}
\newpage
\section*{Fourth Regression: \texttt{mom.hs}, \texttt{mom.iq}  and \texttt{mom.hs:mom.iq}}
Now we'll add an interaction between \texttt{mom.hs} and \texttt{mom.iq}: that is, we'll include a predictor whose value equals the \emph{product} of these two variables. 
The way to express this in R is \texttt{mom.hs:mom.iq}. Consider a child whose mother completed high school (\texttt{mom.hs}=1). For this child, $\texttt{mom.hs:mom.iq} = 1 \cdot \texttt{mom.iq} = \texttt{mom.iq}$. For a child whose mother did \emph{not} complete high school, $\texttt{mom.hs:mom.iq} = 0 \cdot \texttt{mom.iq} = 0$. 
It turns out that adding this interaction allows the two groups (those whose mother completed high school and those whose mother did not) to have regression lines with different slopes.
Since \texttt{mom.iq} is included as in its own right, we allow different intercepts as well.

First we'll run the regression:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{reg4} \hlkwb{<-} \hlkwd{lm}\hlstd{(kid.score} \hlopt{~} \hlstd{mom.hs} \hlopt{+} \hlstd{mom.iq} \hlopt{+} \hlstd{mom.hs}\hlopt{:}\hlstd{mom.iq,} \hlkwc{data} \hlstd{= child)}
\hlkwd{display}\hlstd{(reg4)}
\end{alltt}
\begin{verbatim}
## lm(formula = kid.score ~ mom.hs + mom.iq + mom.hs:mom.iq, data = child)
##               coef.est coef.se
## (Intercept)   -11.48    13.76 
## mom.hs         51.27    15.34 
## mom.iq          0.97     0.15 
## mom.hs:mom.iq  -0.48     0.16 
## ---
## n = 434, k = 4
## residual sd = 17.97, R-Squared = 0.23
\end{verbatim}
\end{kframe}
\end{knitrout}
Rounding, we can summarize the results as follows:
  \begin{eqnarray*}
    (\texttt{mom.hs = 1})\Rightarrow \texttt{kid.score} &=& -11 + 51 \cdot 1 + 1\cdot \texttt{mom.iq} -0.5\cdot 1 \cdot \texttt{mom.iq} +\mbox{error}\\
      &=& 40 + 0.5\cdot \texttt{mom.iq} + \mbox{error}\\ \\
    (\texttt{mom.hs = 0})\Rightarrow \texttt{kid.score} &=& -11 + 51 \cdot 0 + 1\cdot \texttt{mom.iq} -0.5\cdot 0 \cdot \texttt{mom.iq} +\mbox{error}\\
      &=&-11 + 1\cdot \texttt{mom.iq} + \mbox{error}
  \end{eqnarray*}



We can plot the two regression lines as follows. This time we need to allow different slopes \emph{as well as intercepts}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{coef}\hlstd{(reg4)}
\end{alltt}
\begin{verbatim}
##   (Intercept)        mom.hs        mom.iq mom.hs:mom.iq 
##   -11.4820211    51.2682234     0.9688892    -0.4842747
\end{verbatim}
\begin{alltt}
\hlstd{b_HS} \hlkwb{<-} \hlkwd{coef}\hlstd{(reg4)[}\hlnum{3}\hlstd{]} \hlopt{+} \hlkwd{coef}\hlstd{(reg4)[}\hlnum{4}\hlstd{]}
\hlstd{b_no_HS} \hlkwb{<-} \hlkwd{coef}\hlstd{(reg4)[}\hlnum{3}\hlstd{]}
\hlstd{a_HS} \hlkwb{<-} \hlkwd{coef}\hlstd{(reg4)[}\hlnum{1}\hlstd{]} \hlopt{+} \hlkwd{coef}\hlstd{(reg4)[}\hlnum{2}\hlstd{]}
\hlstd{a_no_HS} \hlkwb{<-} \hlkwd{coef}\hlstd{(reg4)[}\hlnum{1}\hlstd{]}
\hlstd{colors} \hlkwb{<-} \hlkwd{ifelse}\hlstd{(child}\hlopt{$}\hlstd{mom.hs} \hlopt{==} \hlnum{0}\hlstd{,} \hlstr{"black"}\hlstd{,} \hlstr{"gray"}\hlstd{)}
\hlkwd{plot}\hlstd{(child}\hlopt{$}\hlstd{mom.iq, child}\hlopt{$}\hlstd{kid.score,} \hlkwc{pch} \hlstd{=} \hlnum{20}\hlstd{,} \hlkwc{xlab} \hlstd{=} \hlstr{'Mother IQ Score'}\hlstd{,}
     \hlkwc{ylab} \hlstd{=} \hlstr{'Child Test Score at Age 3'}\hlstd{,} \hlkwc{col} \hlstd{= colors)}
\hlkwd{abline}\hlstd{(}\hlkwc{a} \hlstd{= a_HS,} \hlkwc{b} \hlstd{= b_HS,} \hlkwc{col} \hlstd{=} \hlstr{'gray'}\hlstd{)}
\hlkwd{abline}\hlstd{(}\hlkwc{a} \hlstd{= a_no_HS,} \hlkwc{b} \hlstd{= b_no_HS,} \hlkwc{col} \hlstd{=} \hlstr{'black'}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-13-1} 

\end{knitrout}

\end{document}
